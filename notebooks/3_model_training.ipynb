{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "In this notebook we mirror the functionality of `src/train_model.py` at a high level. We load the training dataset generated by the label join stage, split it into training and validation folds, fit an XGBoost classifier, and compute performance metrics. The results are comparable to the ones produced by the script.\n",
    "\n",
    "Because the demonstration data included with this repository is synthetic, the exact numbers may differ between runs. Feel free to experiment with the model hyper‑parameters defined in `config/params.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the training data produced by label_join.py\n",
    "data = pd.read_csv('../data/interim/training_data.csv')\n",
    "print(f'Training samples: {len(data)}; Positive fraction: {data.label.mean():.3f}')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label'].values\n",
    "\n",
    "# Simple train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Define XGBoost classifier with reasonable defaults\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n+    "    learning_rate=0.1,\n",
    "    objective='binary:logistic',\n",
    "    n_jobs=4,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the held‑out set\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "roc = roc_auc_score(y_test, proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f'ROC AUC: {roc:.3f}, PR AUC: {pr_auc:.3f}')\n",
    "\n",
    "# Plot ROC and PR curves\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "ax[0].plot(fpr, tpr)\n",
    "ax[0].plot([0,1],[0,1],'--',color='grey')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title(f'ROC (AUC={roc:.2f})')\n",
    "# PR curve\n",
    "ax[1].plot(recall, precision)\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_title(f'PR (AUC={pr_auc:.2f})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save metrics to a dictionary\n",
    "metrics = {'roc_auc': float(roc), 'pr_auc': float(pr_auc)}\n",
    "with open('../data/interim/notebook_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}